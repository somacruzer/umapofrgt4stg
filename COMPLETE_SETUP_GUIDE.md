# åŒæžæž„å¿µèšç±»åˆ†æžç³»ç»Ÿ - å®Œæ•´çŽ¯å¢ƒé…ç½®ä¸Žæ“ä½œæŒ‡å—

## ðŸ“‹ ç³»ç»Ÿæ¦‚è¿°

æœ¬ç³»ç»Ÿæä¾›åŒæžæž„å¿µ(Bipolar Constructs)çš„UMAPèšç±»åˆ†æžå’Œäº¤äº’å¼å¯è§†åŒ–åŠŸèƒ½ï¼Œæ”¯æŒå¤šsurveyæ•°æ®æ•´åˆã€çµæ´»ç­›é€‰å’Œæ•°æ®ä¸‹è½½ã€‚

## ðŸ—ï¸ ç¬¬ä¸€éƒ¨åˆ†ï¼šçŽ¯å¢ƒæ­å»º

### 1.1 ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: macOS, Linux, Windows
- **Pythonç‰ˆæœ¬**: 3.9 - 3.13
- **å†…å­˜**: è‡³å°‘ 8GB RAM
- **å­˜å‚¨**: è‡³å°‘ 2GB å¯ç”¨ç©ºé—´

### 1.2 CondaçŽ¯å¢ƒåˆ›å»º

#### æ­¥éª¤1: åˆ›å»ºæ–°çš„condaçŽ¯å¢ƒ
```bash
# åˆ›å»ºåä¸ºsebrtçš„æ–°çŽ¯å¢ƒï¼Œä½¿ç”¨Python 3.13
conda create -n sebrt python=3.13 -y

# æ¿€æ´»çŽ¯å¢ƒ
conda activate sebrt
```

#### æ­¥éª¤2: å®‰è£…æ ¸å¿ƒä¾èµ–åŒ…
```bash
# å®‰è£…ç§‘å­¦è®¡ç®—åŒ…
conda install numpy=2.1.3 pandas=2.2.3 scipy=1.14.1 -y

# å®‰è£…æœºå™¨å­¦ä¹ åŒ…
conda install scikit-learn=1.6.1 -y

# å®‰è£…å¯è§†åŒ–åŒ…
conda install plotly=6.0.1 -y

# å®‰è£…UMAP (é€šè¿‡pipï¼Œå› ä¸ºcondaç‰ˆæœ¬å¯èƒ½ä¸æ˜¯æœ€æ–°)
pip install umap-learn

# å®‰è£…sentence-transformers
pip install sentence-transformers
```

#### æ­¥éª¤3: éªŒè¯å®‰è£…
```bash
python -c "
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from sklearn.cluster import AgglomerativeClustering
from scipy.spatial import ConvexHull
import umap
print('æ‰€æœ‰åŒ…å®‰è£…æˆåŠŸï¼')
print('NumPyç‰ˆæœ¬:', np.__version__)
print('Pandasç‰ˆæœ¬:', pd.__version__)
print('Plotlyç‰ˆæœ¬:', go.__version__)
"
```

### 1.3 æ›¿ä»£çŽ¯å¢ƒé…ç½®ï¼ˆå¦‚æžœä¸Šè¿°æ–¹æ³•æœ‰é—®é¢˜ï¼‰

#### ä½¿ç”¨requirements.txtæ–¹å¼ï¼š
```bash
# åˆ›å»ºçŽ¯å¢ƒ
conda create -n sebrt python=3.13 -y
conda activate sebrt

# åˆ›å»ºrequirements.txtæ–‡ä»¶
cat > requirements.txt << 'EOF'
numpy==2.1.3
pandas==2.2.3
scipy==1.14.1
scikit-learn==1.6.1
plotly==6.0.1
umap-learn>=0.5.3
sentence-transformers>=2.2.0
EOF

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

## ðŸ“‚ ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®å‡†å¤‡

### 2.1 æ•°æ®æ–‡ä»¶ç»“æž„

ç³»ç»Ÿéœ€è¦ä»¥ä¸‹è¾“å…¥æ–‡ä»¶ï¼š

#### å¿…éœ€æ–‡ä»¶ï¼š
1. **åŽŸå§‹UMAPåæ ‡æ–‡ä»¶** (`../umap_coordinates.csv`)
```csv
index,pid,agg_cluster,x,y
0,01903166-988A-4E13-B883-878582894D29,21,4.7684383,4.328492
1,01903166-988A-4E13-B883-878582894D29,25,1.4595591,2.9183085
...
```

2. **Surveyæ˜ å°„æ–‡ä»¶** (`pid to survey.csv`)
```csv
conversation_id,construct,construct_bipolar,survey_id,survey_name,survey_endTime
01903166-988A-4E13-B883-878582894D29,Traditional,Innovative,b2b8d8e1-3557,Construct Elaboration,2025-01-08 07:37:18
01903166-988A-4E13-B883-878582894D29,Richness,Lightness,b2b8d8e1-3557,Construct Elaboration,2025-01-08 07:37:18
...
```

### 2.2 æ•°æ®æ ¼å¼è¯´æ˜Ž

#### UMAPåæ ‡æ–‡ä»¶æ ¼å¼ï¼š
- `index`: æ•°æ®ç‚¹ç´¢å¼•
- `pid`: å¯¹è¯ID (conversation_id)
- `agg_cluster`: åŽŸå§‹èšç±»ç¼–å·
- `x`, `y`: UMAPåæ ‡

#### Surveyæ˜ å°„æ–‡ä»¶æ ¼å¼ï¼š
- `conversation_id`: å¯¹è¯ID (å¯¹åº”pid)
- `construct`: åŒæžæž„å¿µçš„Aæž (pole_a)
- `construct_bipolar`: åŒæžæž„å¿µçš„Bæž (pole_b)
- `survey_id`: è°ƒæŸ¥ID
- `survey_name`: è°ƒæŸ¥åç§°
- `survey_endTime`: è°ƒæŸ¥ç»“æŸæ—¶é—´

### 2.3 æ•°æ®å‡†å¤‡æ£€æŸ¥æ¸…å•

```bash
# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls -la "../umap_coordinates.csv"
ls -la "pid to survey.csv"

# æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå†…å®¹
head -5 "../umap_coordinates.csv"
head -5 "pid to survey.csv"

# æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
python -c "
import pandas as pd
coords = pd.read_csv('../umap_coordinates.csv')
survey = pd.read_csv('pid to survey.csv')
print(f'åæ ‡æ•°æ®: {len(coords)} è¡Œ')
print(f'Surveyæ•°æ®: {len(survey)} è¡Œ')
print(f'å”¯ä¸€å¯¹è¯ID: {coords[\"pid\"].nunique()}')
print(f'Surveyç±»åž‹: {survey[\"survey_name\"].nunique()}')
"
```

## ðŸš€ ç¬¬ä¸‰éƒ¨åˆ†ï¼šç³»ç»Ÿè¿è¡Œ

### 3.1 ä¸‹è½½å’Œå‡†å¤‡è„šæœ¬

```bash
# ç¡®ä¿åœ¨æ­£ç¡®ç›®å½•
cd "/Users/liuyunxing/Documents/upload/sentence-transformers/bipolar analysis output/final version of constructs cluster"

# ç¡®è®¤è„šæœ¬å­˜åœ¨
ls -la final_constructs_cluster_analysis.py
```

### 3.2 è¿è¡Œåˆ†æžç³»ç»Ÿ

```bash
# æ¿€æ´»çŽ¯å¢ƒ
conda activate sebrt

# è¿è¡Œå®Œæ•´åˆ†æž
python final_constructs_cluster_analysis.py
```

### 3.3 é¢„æœŸè¾“å‡º

è¿è¡ŒæˆåŠŸåŽä¼šç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

#### æ•°æ®æ–‡ä»¶ï¼š
- `complete_processed_dataset.csv` - å®Œæ•´å¤„ç†åŽçš„æ•°æ®é›†
- `constructs_cluster_dataset.csv` - ç”¨æˆ·ä¸‹è½½çš„ç²¾ç®€æ•°æ®é›†
- `cluster_analysis_statistics.csv` - èšç±»åˆ†æžç»Ÿè®¡
- `survey_analysis_statistics.csv` - è°ƒæŸ¥åˆ†æžç»Ÿè®¡
- `dataset_summary.txt` - æ•°æ®é›†æ‘˜è¦

#### å¯è§†åŒ–æ–‡ä»¶ï¼š
- `interactive_constructs_cluster_visualization.html` - äº¤äº’å¼å¯è§†åŒ–

## ðŸ“Š ç¬¬å››éƒ¨åˆ†ï¼šåŠŸèƒ½ä½¿ç”¨æŒ‡å—

### 4.1 äº¤äº’å¼å¯è§†åŒ–

#### æ‰“å¼€å¯è§†åŒ–ï¼š
```bash
# åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€
open interactive_constructs_cluster_visualization.html
# æˆ–ç›´æŽ¥åŒå‡»HTMLæ–‡ä»¶
```

#### ä¸»è¦åŠŸèƒ½ï¼š
1. **ðŸ  Show All Data** - æ˜¾ç¤ºæ‰€æœ‰æ•°æ®
2. **ðŸ“Š Clusters Only** - ä»…æ˜¾ç¤ºèšç±»
3. **ðŸŽ¯ Surveys Only** - ä»…æ˜¾ç¤ºè°ƒæŸ¥æ•°æ®
4. **ðŸ” Highlight [Survey]** - é«˜äº®ç‰¹å®šè°ƒæŸ¥
5. **ðŸ“ Cluster X Only** - æ˜¾ç¤ºç‰¹å®šèšç±»

### 4.2 æ•°æ®ä¸‹è½½åŠŸèƒ½

#### ä¸‹è½½çš„CSVæ–‡ä»¶åŒ…å«ï¼š
- `User_ID`: ç”¨æˆ·æ ‡è¯† (åŸºäºŽconversation_id)
- `Survey_ID`: è°ƒæŸ¥ID
- `Survey_Name`: è°ƒæŸ¥åç§°
- `Pole_A`: åŒæžæž„å¿µAæž
- `Pole_B`: åŒæžæž„å¿µBæž  
- `Cluster`: èšç±»ç¼–å· (0-6)
- `UMAP_X`: Xåæ ‡
- `UMAP_Y`: Yåæ ‡

#### ä½¿ç”¨ä¸‹è½½æ•°æ®ï¼š
```python
import pandas as pd

# è¯»å–ä¸‹è½½çš„æ•°æ®
df = pd.read_csv('constructs_cluster_dataset.csv')

# åŸºæœ¬ç»Ÿè®¡
print(df.describe())
print(df['Cluster'].value_counts())
print(df['Survey_Name'].value_counts())

# åˆ†æžç¤ºä¾‹
cluster_survey = pd.crosstab(df['Cluster'], df['Survey_Name'])
print(cluster_survey)
```

## ðŸ”§ ç¬¬äº”éƒ¨åˆ†ï¼šæ•…éšœæŽ’é™¤

### 5.1 å¸¸è§é—®é¢˜

#### é—®é¢˜1: åŒ…å¯¼å…¥é”™è¯¯
```bash
# è§£å†³æ–¹æ¡ˆï¼šé‡æ–°å®‰è£…åŒ…
conda activate sebrt
pip install --upgrade plotly pandas numpy scikit-learn scipy
```

#### é—®é¢˜2: æ–‡ä»¶è·¯å¾„é”™è¯¯
```bash
# æ£€æŸ¥æ–‡ä»¶è·¯å¾„
pwd
ls -la "../umap_coordinates.csv"
ls -la "pid to survey.csv"
```

#### é—®é¢˜3: å†…å­˜ä¸è¶³
```bash
# ç›‘æŽ§å†…å­˜ä½¿ç”¨
python -c "
import psutil
print(f'å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / 1024**3:.1f} GB')
"
```

### 5.2 çŽ¯å¢ƒé‡ç½®

å¦‚æžœçŽ¯å¢ƒå‡ºçŽ°é—®é¢˜ï¼Œå¯ä»¥å®Œå…¨é‡ç½®ï¼š

```bash
# åˆ é™¤æ—§çŽ¯å¢ƒ
conda deactivate
conda env remove -n sebrt

# é‡æ–°åˆ›å»ºçŽ¯å¢ƒ
conda create -n sebrt python=3.13 -y
conda activate sebrt

# é‡æ–°å®‰è£…åŒ…
pip install numpy pandas scipy scikit-learn plotly umap-learn sentence-transformers
```

### 5.3 æ•°æ®éªŒè¯

```bash
# è¿è¡Œæ•°æ®éªŒè¯è„šæœ¬
python -c "
import pandas as pd
import numpy as np

# æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
try:
    coords = pd.read_csv('../umap_coordinates.csv')
    survey = pd.read_csv('pid to survey.csv')
    
    print('âœ… æ–‡ä»¶è¯»å–æˆåŠŸ')
    print(f'åæ ‡æ•°æ®: {len(coords)} è¡Œ, {len(coords.columns)} åˆ—')
    print(f'Surveyæ•°æ®: {len(survey)} è¡Œ, {len(survey.columns)} åˆ—')
    
    # æ£€æŸ¥å¿…éœ€åˆ—
    required_coords = ['index', 'pid', 'agg_cluster', 'x', 'y']
    required_survey = ['conversation_id', 'construct', 'construct_bipolar', 'survey_name']
    
    for col in required_coords:
        if col not in coords.columns:
            print(f'âŒ åæ ‡æ–‡ä»¶ç¼ºå°‘åˆ—: {col}')
        else:
            print(f'âœ… åæ ‡æ–‡ä»¶åŒ…å«: {col}')
    
    for col in required_survey:
        if col not in survey.columns:
            print(f'âŒ Surveyæ–‡ä»¶ç¼ºå°‘åˆ—: {col}')
        else:
            print(f'âœ… Surveyæ–‡ä»¶åŒ…å«: {col}')
            
    print('æ•°æ®éªŒè¯å®Œæˆ!')
    
except Exception as e:
    print(f'âŒ é”™è¯¯: {e}')
"
```

## ðŸ“ˆ ç¬¬å…­éƒ¨åˆ†ï¼šé«˜çº§ä½¿ç”¨

### 6.1 è‡ªå®šä¹‰èšç±»æ•°é‡

å¦‚éœ€ä¿®æ”¹èšç±»æ•°é‡ï¼ˆé»˜è®¤7ä¸ªï¼‰ï¼Œç¼–è¾‘è„šæœ¬ä¸­çš„å‚æ•°ï¼š

```python
# åœ¨final_constructs_cluster_analysis.pyä¸­æ‰¾åˆ°è¿™è¡Œï¼š
clustering = AgglomerativeClustering(n_clusters=7, linkage='ward')

# ä¿®æ”¹ä¸ºæ‰€éœ€æ•°é‡ï¼Œä¾‹å¦‚5ä¸ªèšç±»ï¼š
clustering = AgglomerativeClustering(n_clusters=5, linkage='ward')
```

### 6.2 æ‰¹å¤„ç†å¤šä¸ªæ•°æ®é›†

```bash
# åˆ›å»ºæ‰¹å¤„ç†è„šæœ¬
cat > batch_process.py << 'EOF'
import os
import subprocess

datasets = [
    'dataset1',
    'dataset2', 
    'dataset3'
]

for dataset in datasets:
    print(f"å¤„ç†æ•°æ®é›†: {dataset}")
    os.chdir(f"/path/to/{dataset}")
    subprocess.run(['python', 'final_constructs_cluster_analysis.py'])
    print(f"å®Œæˆ: {dataset}")
EOF

python batch_process.py
```

### 6.3 æ€§èƒ½ä¼˜åŒ–

å¯¹äºŽå¤§åž‹æ•°æ®é›†ï¼Œå¯ä»¥è°ƒæ•´è¿™äº›å‚æ•°ï¼š

```python
# å‡å°‘UMAPè®¡ç®—å¤æ‚åº¦
umap_reducer = umap.UMAP(
    n_neighbors=15,    # é»˜è®¤15ï¼Œå‡å°‘åˆ°10-15
    min_dist=0.1,      # é»˜è®¤0.1
    n_components=2,    # ä¿æŒ2D
    random_state=42,
    metric='cosine'
)

# ä½¿ç”¨æ›´å¿«çš„èšç±»ç®—æ³•
from sklearn.cluster import KMeans
clustering = KMeans(n_clusters=7, random_state=42)
```

## ðŸ“ž æ”¯æŒä¸Žè”ç³»

å¦‚é‡é—®é¢˜ï¼Œè¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
1. é”™è¯¯ä¿¡æ¯çš„å®Œæ•´æˆªå›¾
2. Pythonç‰ˆæœ¬å’ŒåŒ…ç‰ˆæœ¬
3. è¾“å…¥æ•°æ®æ–‡ä»¶çš„å‰å‡ è¡Œ
4. è¿è¡ŒçŽ¯å¢ƒ(macOS/Linux/Windows)

---

*æœ€åŽæ›´æ–°: 2025å¹´7æœˆ24æ—¥*
